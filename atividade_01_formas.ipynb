{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joaoBernardinoo/formas-research/blob/main/atividade_01_formas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ú® Etiquetagem de Classes Gramaticais em Palavras Baseadas em Regras ‚ú®\n",
        "\n",
        "Este projeto implementa um **etiquetador morfol√≥gico baseado em regras** para o idioma portugu√™s, inspirado no trabalho cl√°ssico de **Brill (1992)**. Utilizando o renomado **corpus Bosque**, o objetivo √© por em pr√°tica o conhecimento adquir√≠do durante minha inicia√ß√£o cient√≠fica, como tamb√©m testar hip√≥tese de que, mesmo sendo o portugu√™s uma l√≠ngua mais verbosa que o ingl√™s, podemos aplicar etiquetas gramaticais corretas utilizando os tr√™s √∫ltimos caracteres dos tokens etiquetados em um corpus padr√£o ouro **( Bosque )**.\n",
        "\n",
        "üîç **Destaques do Projeto**:\n",
        "- Utiliza t√©cnicas de **Processamento de Linguagem Natural (PLN)**.\n",
        "- Baseado em regras lingu√≠sticas para an√°lise morfol√≥gica.\n",
        "- Testa a efici√™ncia de sufixos na **etiquetagem gramatical** em portugu√™s.\n",
        "\n",
        "üìä **Corpus Utilizado**:\n",
        "- **Bosque** (um dos maiores e mais completos corpora da l√≠ngua portuguesa).\n",
        "\n",
        "---\n",
        "\n",
        "### üìö Refer√™ncias\n",
        "\n",
        "BRILL, E. *A Simple Rule-Based Part of Speech Tagger*. Proceedings of the Third Conference on Applied Natural Language Processing. **ANLC ‚Äô92**. USA: Association for Computational Linguistics, 1992. Dispon√≠vel em: [https://doi.org/10.3115/974499.974526](https://doi.org/10.3115/974499.974526)\n",
        "\n"
      ],
      "metadata": {
        "id": "RYctk8nD4aVC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTIczJD7ItVX"
      },
      "source": [
        "import pickle\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import copy\n",
        "import itertools as it\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sytvxxbuSfSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install conllu\n",
        "!wget http://marlovss.work.gd:8080/tomorrow/aula2/bosque.conllu"
      ],
      "metadata": {
        "id": "HnkdNxnCmtn8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60b74a28-5691-4b95-f455-269f7c4dc32a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: conllu in /usr/local/lib/python3.10/dist-packages (6.0.0)\n",
            "--2024-10-25 17:39:55--  http://marlovss.work.gd:8080/tomorrow/aula2/bosque.conllu\n",
            "Resolving marlovss.work.gd (marlovss.work.gd)... 177.180.148.12\n",
            "Connecting to marlovss.work.gd (marlovss.work.gd)|177.180.148.12|:8080... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11291250 (11M)\n",
            "Saving to: ‚Äòbosque.conllu.2‚Äô\n",
            "\n",
            "bosque.conllu.2     100%[===================>]  10.77M  1.46MB/s    in 8.5s    \n",
            "\n",
            "2024-10-25 17:40:04 (1.27 MB/s) - ‚Äòbosque.conllu.2‚Äô saved [11291250/11291250]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import conllu\n",
        "import itertools as it\n",
        "\n",
        "class AttributeDict(dict):\n",
        "    __getattr__ = dict.__getitem__\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n",
        "class CoNLLU:\n",
        "   def __init__(self, files):\n",
        "      self.words = []\n",
        "      self.sentences = []\n",
        "      for f in files:\n",
        "         parsed = conllu.parse(open(f).read())\n",
        "         sents = [[AttributeDict(form = token['form'], lemma=token['lemma'],pos=token['upos'],feats=token['feats']) for token in tokenlist if token['upos']!='_'] for tokenlist in parsed]\n",
        "         self.sentences.extend(sents)\n",
        "         self.words.extend([word for sent in sents for word in sent])\n",
        "      self.pos_tags = set([word.pos for word in self.words])\n",
        "      self.feats_dict ={pos:set(it.chain.from_iterable([list(word.feats.keys()) for word in self.words if word.pos==pos and word.feats!= None])) for pos in self.pos_tags}\n"
      ],
      "metadata": {
        "id": "6whsxfkKmTnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-TW_mVFMzPK",
        "outputId": "4ccf2845-6f13-4e8b-81e3-940d91b84df2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bosque = CoNLLU(files=[\"bosque.conllu\"])"
      ],
      "metadata": {
        "id": "JY5mcN4Em2zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# aqui train_data, patch_data e test_data s√£o o corpus \"bosque\" particionado por suas sentenƒáas, n√£o palavras\n",
        "# deve-se verificar se todas as partiƒá√µes abrangem todas as \"universal pos tags\"\n",
        "\n",
        "train_data, temp_data = train_test_split(bosque.sentences, test_size=0.1, random_state=42) # 90% train, 10% temp\n",
        "patch_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42) # Split the 10% into 5% patch and 5% test\n",
        "\n",
        "print(f\"Training data size: {len(train_data)}\")\n",
        "print(f\"Patch data size: {len(patch_data)}\")\n",
        "print(f\"Test data size: {len(test_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X670ynEPLr7D",
        "outputId": "ac658099-673b-4102-8ba4-4391d8632f93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data size: 6316\n",
            "Patch data size: 351\n",
            "Test data size: 351\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-60soNSkHQu",
        "outputId": "b07bc70e-2f52-4bc7-dfbf-fe26f41c946c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'form': 'Afirmou', 'lemma': 'afirmar', 'pos': 'VERB', 'feats': {'Mood': 'Ind', 'Number': 'Sing', 'Person': '3', 'Tense': 'Past', 'VerbForm': 'Fin'}}, {'form': 'que', 'lemma': 'que', 'pos': 'SCONJ', 'feats': None}, {'form': 'o', 'lemma': 'o', 'pos': 'DET', 'feats': {'Definite': 'Def', 'Gender': 'Masc', 'Number': 'Sing', 'PronType': 'Art'}}, {'form': 'conjunto', 'lemma': 'conjunto', 'pos': 'NOUN', 'feats': {'Gender': 'Masc', 'Number': 'Sing'}}, {'form': 'de', 'lemma': 'de', 'pos': 'ADP', 'feats': None}, {'form': 'fatos', 'lemma': 'fato', 'pos': 'NOUN', 'feats': {'Gender': 'Masc', 'Number': 'Plur'}}, {'form': ',', 'lemma': ',', 'pos': 'PUNCT', 'feats': None}, {'form': 'em', 'lemma': 'em', 'pos': 'ADP', 'feats': None}, {'form': 'princ√≠pio', 'lemma': 'princ√≠pio', 'pos': 'NOUN', 'feats': {'Gender': 'Masc', 'Number': 'Sing'}}, {'form': ',', 'lemma': ',', 'pos': 'PUNCT', 'feats': None}, {'form': 'aponta', 'lemma': 'apontar', 'pos': 'VERB', 'feats': {'Mood': 'Ind', 'Number': 'Sing', 'Person': '3', 'Tense': 'Pres', 'VerbForm': 'Fin'}}, {'form': 'o', 'lemma': 'o', 'pos': 'DET', 'feats': {'Definite': 'Def', 'Gender': 'Masc', 'Number': 'Sing', 'PronType': 'Art'}}, {'form': 'envolvimento', 'lemma': 'envolvimento', 'pos': 'NOUN', 'feats': {'Gender': 'Masc', 'Number': 'Sing'}}, {'form': 'de', 'lemma': 'de', 'pos': 'ADP', 'feats': None}, {'form': 'Qu√©rcia', 'lemma': 'Qu√©rcia', 'pos': 'PROPN', 'feats': {'Gender': 'Masc', 'Number': 'Sing'}}, {'form': '.', 'lemma': '.', 'pos': 'PUNCT', 'feats': None}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_words = sorted([word for sentence in train_data for word in sentence], key=lambda x: x.form)"
      ],
      "metadata": {
        "id": "PJFWs_VVkK3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "suffixes = set([word.form.lower()[-3:] for word in train_words])"
      ],
      "metadata": {
        "id": "7Hm33XPLszLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# O artigo utiliza 3 ultimos caracteres do token do corpus anotado na lingua inglesa,\n",
        "# hip√≥tese ( precisa testar ): a l√≠ngua portuguesa √© mais verbosa, seria 3 caracteres o suficiente\n",
        "# para, por exemplo, contemplar todas as conjugaƒá√µes verbais??\n",
        "\n",
        "try:\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/suf_to_tag.pkl', 'rb') as f:\n",
        "        suf_to_tag = pickle.load(f)\n",
        "except FileNotFoundError:\n",
        "    print(\"Arquivo nao encontrado, extraindo os sufixos...\")\n",
        "    suf_to_tag = {suf: FreqDist([word.pos for word in train_words if word.form.lower()[-3:] == suf]).max() for suf in suffixes}\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/suf_to_tag.pkl', 'wb') as f:\n",
        "        pickle.dump(suf_to_tag, f)"
      ],
      "metadata": {
        "id": "8TXGTGCISoZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rules = {\n",
        "    'ADJ': [],\n",
        "    'ADP': [],\n",
        "    'ADV': [],\n",
        "    'AUX': [],\n",
        "    'CCONJ': [],\n",
        "    'DET': [],\n",
        "    'INTJ': [],\n",
        "    'NOUN': [],\n",
        "    'NUM': [],\n",
        "    'PART': [],\n",
        "    'PRON': [],\n",
        "    'PROPN': [],\n",
        "    'PUNCT': [],\n",
        "    'SCONJ': [],\n",
        "    'SYM': [],\n",
        "    'VERB': [],\n",
        "    'X': []\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(list(rules.items()), columns=['pos_tag', 'token'])\n",
        "df['token'] = df['token'].apply(set)\n"
      ],
      "metadata": {
        "id": "X1fphrrOYSFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sents = [[word.form for word in sent] for sent in train_data]\n",
        "patch_sents = [[word.form for word in sent] for sent in patch_data]\n",
        "patch_gold = [[(word.form,word.pos) for word in sent] for sent in patch_data]\n",
        "test_gold  = [[(word.form.lower(),word.pos) for word in sent] for sent in test_data]"
      ],
      "metadata": {
        "id": "Vs8Qp2D_lND0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# primeiro vamos etiquetar o patch\n",
        "# quantificando as vezes ao inv√©s de etiquetar tag b, etiquetou tag a\n",
        "# < tagA, tagB, number >\n",
        "\n",
        "def lexic_tag(tokens):\n",
        "  tagged = []\n",
        "  for token in tokens:\n",
        "    if token.lower()[-3:] in suffixes:\n",
        "       tagged.append([token,suf_to_tag[token.lower()[-3:]]])\n",
        "    else:\n",
        "       tagged.append([token,\"_\"])\n",
        "  return tagged\n",
        "\n",
        "# has to add < tagA, tagB, number > to a list, when a word is mistagged with a tagA\n",
        "# when it should be tagB\n",
        "\n",
        "def lexic_tag_error(predicted,gold):\n",
        "  mistagged = []\n",
        "  for j in range(len(gold)):\n",
        "    for i in range(len(gold[j])):\n",
        "      tagA = predicted[j][i][1]\n",
        "      tagB = gold[j][i][1]\n",
        "      if tagA != tagB:\n",
        "        # caso o elemento < tagA,tagB, number > exista, incremente number\n",
        "        # caso contr√°rio adicione < tagA, tagB,  1 > na lista\n",
        "        found = False\n",
        "        for k in range(len(mistagged)):\n",
        "            if mistagged[k][0] == tagA and mistagged[k][1] == tagB:\n",
        "                mistagged[k] = (tagA, tagB, mistagged[k][2] + 1)\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            mistagged.append((tagA, tagB, 1))\n",
        "\n",
        "  return mistagged"
      ],
      "metadata": {
        "id": "ks1GEJIGqJeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patch_pred = [lexic_tag(sent) for sent in patch_sents]"
      ],
      "metadata": {
        "id": "ebxqb5E2xzCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(patch_pred[0][1])\n",
        "print(patch_gold[0][1])\n",
        "print(f\"Aqui houve divergencia entre os dois, devemos adicionar \\n< {patch_pred[0][1][1]},{patch_gold[0][1][1]}, +1> na lista de triplas\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-HvACB5zfeF",
        "outputId": "6a72f30e-31c4-435b-f8ce-566ab3a9b348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['substitutivo', 'ADJ']\n",
            "('substitutivo', 'NOUN')\n",
            "Aqui houve divergencia entre os dois, devemos adicionar \n",
            "< ADJ,NOUN, +1> na lista de triplas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "triples = lexic_tag_error(patch_pred,patch_gold)\n",
        "triples[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rTONOb3y9A-",
        "outputId": "0547ebb4-a288-4376-8a0c-f7cd5ae7c42b"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ADJ', 'NOUN', 61),\n",
              " ('DET', 'ADP', 125),\n",
              " ('ADV', 'NOUN', 61),\n",
              " ('PROPN', 'NOUN', 67),\n",
              " ('VERB', 'NOUN', 120)]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_triples = sorted(triples, key=lambda x: x[2], reverse=True)\n",
        "sorted_triples[:5]"
      ],
      "metadata": {
        "id": "9e8c5RzSDXvb",
        "outputId": "79353631-84a4-40e7-eb71-65b3841e3fc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('NOUN', 'PROPN', 225),\n",
              " ('NOUN', 'ADJ', 159),\n",
              " ('DET', 'ADP', 125),\n",
              " ('VERB', 'NOUN', 120),\n",
              " ('PRON', 'SCONJ', 86)]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_errors = sum([triple[2] for triple in triples])\n",
        "print(total_errors)"
      ],
      "metadata": {
        "id": "UgqIXvH2nzbG"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utilizaremos os templates abaixo para gerar os patches:\n",
        "(Brill, 1992)\n",
        "Change tag a to tag b when:\n",
        "1. The preceding (following) word is tagged z.\n",
        "2. The word two before (after) is tagged z.\n",
        "3. One of the two preceding (following) words is tagged\n",
        "Z.\n",
        "4. One of the three preceding (following) words is\n",
        "tagged z.\n",
        "5. The preceding word is tagged z and the following\n",
        "word is tagged w.\n",
        "6. The preceding (following) word is tagged z and the\n",
        "word two before (after) is tagged w.\n",
        "7. The current word is (is not) capitalized.\n",
        "8. The previous word is (is not) capitalized."
      ],
      "metadata": {
        "id": "kwembB5K4FqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_train_words(train_words):\n",
        "     \"\"\" Dicion√°rio para checagem r√°pida das palavras do corpus de treino \"\"\"\n",
        "     word_to_tags = {}\n",
        "     for word_data in train_words:\n",
        "       form = word_data['form']\n",
        "       pos_tag = word_data['pos']\n",
        "       word_to_tags.setdefault(form, set()).add(pos_tag)\n",
        "     return word_to_tags\n",
        "\n",
        "word_to_tags_lookup = preprocess_train_words(train_words)"
      ],
      "metadata": {
        "id": "Q-PMkM0dJGZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patches = []"
      ],
      "metadata": {
        "id": "rrBQIFK6CRzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conditions = [\n",
        "    \"NEXT-TAG\",\n",
        "    \"PREV-TAG\",\n",
        "    \"NEXT-2-TAG\",\n",
        "    \"PREV-2-TAG\",\n",
        "    \"NEXT-1-OR-2-TAG\",\n",
        "    \"PREV-1-OR-2-TAG\",\n",
        "    \"NEXT-1-OR-2-OR-3-TAG\",\n",
        "    \"PREV-1-OR-2-OR-3-TAG\",\n",
        "    \"PREV-TAG-NEXT-TAG\",\n",
        "    \"PREV-TAG-NEXT-2-TAG\",\n",
        "    \"PREV-2-TAG-NEXT-TAG\",\n",
        "    \"IS-CAPITALIZED\",\n",
        "    \"IS-NOT-CAPITALIZED\",\n",
        "    \"PREV-IS-CAPITALIZED\",\n",
        "    \"PREV-IS-NOT-CAPITALIZED\",\n",
        "]"
      ],
      "metadata": {
        "id": "_pMXIWGaG91n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_templates(tagA, pos_tags,conditions):\n",
        "    templates = []\n",
        "    for tagB,tagC,conditions in it.product(pos_tags,pos_tags,conditions):\n",
        "        templates.append(PatchTemplate(tagA,tagB,conditions,tagC))\n",
        "        # 1. The preceding (following) word is tagged z.\n",
        "        # 2. The word two before (after) is tagged z.\n",
        "        # 3. One of the two preceding (following) words is tagged Z.\n",
        "        # 4. One of the three preceding (following) words is tagged z.\n",
        "        # 5. The preceding word is tagged z and the following word is tagged w.\n",
        "        # 6. The preceding (following) word is tagged z and the word two before (after) is tagged w.\n",
        "        # 7. The current word is (is not) capitalized.\n",
        "        # 8. The previous word is (is not) capitalized.\n",
        "    return templates\n",
        "\n",
        "pos_tags = {'ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X'}\n",
        "templates = generate_templates(pos_tags)\n",
        "\n",
        "conditions = [\"NEXT-TAG\",\"PREV-TAG\"]\n",
        "# cada objeto da classe patch template tem 4 attributos ex:\n",
        "#  VERB PREP NEXT-TAG DET\n",
        "# The first patch states that if a word is tagged VERB\n",
        "# and the following word is tagged DET, then switch the\n",
        "# tag from VERB to PREP.\n",
        "\n",
        "class PatchTemplate():\n",
        "  def __init__(self, tagA, tagB, cond, tagC = \"_\"):\n",
        "    self.current = tagA\n",
        "    self.patch = tagB\n",
        "    self.cond = cond\n",
        "    self.next = tagC\n",
        "\n",
        "  def __str__(self):\n",
        "    return f\"{self.current} {self.next} {self.cond} {self.patch}\"\n",
        "  #   A patch which\n",
        "  # changes the tagging of a word from a to b only applies\n",
        "  # if the word has been tagged b somewhere in the training\n",
        "  # corpus.\n",
        "  def canTag(self, word):\n",
        "    \"\"\"Checa se o patch pode ser aplicando usando o dicionario de consulta\"\"\"\n",
        "    return self.next in word_to_tags_lookup.get(word.lower(), set())\n",
        "\n",
        "  def apply(self,predicted):\n",
        "    predicted_copy = copy.deepcopy(predicted)\n",
        "    if self.canTag(predicted_copy[0]):\n",
        "        predicted_copy[1] = self.patch\n",
        "\n",
        "        patched_error = lexic_tag_error([predicted_copy], patch_gold)\n",
        "        patched_error_sum = sum(err[2] for err in patched_error)\n",
        "\n",
        "        print(\"Original Error:\", total_errors)\n",
        "        print(\"Patched Error:\", patched_error_sum)\n",
        "\n",
        "        if patched_error_sum < total_errors:\n",
        "            predicted[:] = predicted_copy\n",
        "            patches.append(self)\n",
        "    return\n"
      ],
      "metadata": {
        "id": "wPJ9uoiv66FB"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "msStwa3GyiI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patch_pred[0][0]\n"
      ],
      "metadata": {
        "id": "ONeD0oiED0WN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd9a20ad-ea3f-4b64-f69b-b69468d50c33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O', 'DET']"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tag(tokens):\n",
        "  tagged = []\n",
        "  for token in tokens:\n",
        "    if token.lower()[-3:] in suffixes:\n",
        "       tagged.append((token,suf_to_tag[token.lower()[-3:]]))\n",
        "    else:\n",
        "       tagged.append((token,\"_\"))\n",
        "  return tagged"
      ],
      "metadata": {
        "id": "W4ytM09SYVky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(predicted,gold):\n",
        "\n",
        "   acertos = len([predicted[i][j][1] for i in range(len(gold)) for j in range(len(gold[i])) if predicted[i][j][1]==gold[i][j][1]])\n",
        "   totais = sum([len(sent) for sent in gold])\n",
        "   return acertos/totais\n",
        "\n",
        "def abrangencia(predicted,gold):\n",
        "  tagged_tokens = 0\n",
        "\n",
        "  for sent in predicted:\n",
        "    for _, predicted_tag in sent:\n",
        "      if predicted_tag != \"_\":\n",
        "        tagged_tokens += 1\n",
        "  total_tokens = 0\n",
        "\n",
        "  for sent in gold:\n",
        "    for _, gold_tag in sent:\n",
        "      if gold_tag != \"_\":\n",
        "        total_tokens += 1\n",
        "  return tagged_tokens / total_tokens\n",
        "\n",
        "def F(predicted,gold):\n",
        "  return 2 * (abrangencia(predicted,gold) * accuracy(predicted,gold)) / (abrangencia(predicted,gold) + accuracy(predicted,gold))"
      ],
      "metadata": {
        "id": "wKeAIlsdOArQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://marlovss.work.gd:8080/tomorrow/aula2/test.conllu"
      ],
      "metadata": {
        "id": "6xsx4167f0Az",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3c25619-4efc-46f5-dbc2-e8154eb9b211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-25 17:40:26--  http://marlovss.work.gd:8080/tomorrow/aula2/test.conllu\n",
            "Resolving marlovss.work.gd (marlovss.work.gd)... 177.180.148.12\n",
            "Connecting to marlovss.work.gd (marlovss.work.gd)|177.180.148.12|:8080... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1819980 (1.7M)\n",
            "Saving to: ‚Äòtest.conllu.2‚Äô\n",
            "\n",
            "test.conllu.2       100%[===================>]   1.74M  1.23MB/s    in 1.4s    \n",
            "\n",
            "2024-10-25 17:40:30 (1.23 MB/s) - ‚Äòtest.conllu.2‚Äô saved [1819980/1819980]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = CoNLLU(files=[\"test.conllu\"])\n",
        "test_sents = [[word.form for word in sent] for sent in test.sentences]\n",
        "gold = [[(word.form.lower(),word.pos) for word in sent] for sent in test.sentences]\n",
        "predicted = [tag(sent) for sent in test_sents]"
      ],
      "metadata": {
        "id": "1FchDpmwgr1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(train,test):\n",
        "  gold = [[(word.form.lower(),word.pos) for word in sent] for sent in test]\n",
        "  predicted = [tag(sent) for sent in train]\n",
        "  return {\n",
        "        'accuracy': accuracy(predicted,gold),\n",
        "        'coverage': abrangencia(predicted,gold),\n",
        "        \"F\" : F(predicted,gold)\n",
        "}"
      ],
      "metadata": {
        "id": "qGs1VyeFKv1j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}