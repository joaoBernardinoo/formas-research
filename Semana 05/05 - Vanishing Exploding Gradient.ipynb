{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCi_bd9wr96"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/alan-barzilay/NLPortugues/master/imagens/logo_nlportugues.png\"   width=\"150\" align=\"right\">\n",
        "\n",
        "\n",
        "# Lista 5 - Vanishing & Exploding Gradient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "______________\n"
      ],
      "metadata": {
        "id": "roQtHd9Sw2da"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXMOjXMowr98"
      },
      "source": [
        "\n",
        "Nessa lista exploraremos alguns problemas que podemos encontrar ao treinarmos uma rede recorrente. Esses problemas não são únicos das redes recorrentes, qualquer rede profunda pode sofrer de vanishing e exploding gradient mas as redes recorrentes são especialmente instaveis devido a utilização da mesma matriz de pesos repetidas vezes.\n",
        "\n",
        "Começaremos explorando o exploding gradient e alguns de seus sintomas, em seguida utilizaremos gradient cliping para combate-lo.\n",
        "Por fim estudaremos uma rede que sofre de vanishing gradient e utilizaremos o [TensorBoard](https://github.com/tensorflow/tensorboard) para visualizar os gradientes e pesos da rede para entender melhor sua dinâmica. Exploraremos tambem algumas maneiras de combater o vanishing gradient e como elas alteram a dinâmica da rede através do TensorBoard.\n",
        "\n",
        "\n",
        "\n",
        "**Nota:** Nesta aula utilizaremos um callback especial para visualizarmos os gradientes e pesos da rede. Para garantir que tudo funcione corretamente utilizaremos uma versão mais antiga do tensorflow já que a versão 2.3.0 introduziu mudanças que quebram esse callback."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install 'tensorflow<2.3' --force-reinstall"
      ],
      "metadata": {
        "id": "_WeEJVPhzUNP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b22b4cef-153e-431d-c0fb-e13e581e085e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow<2.3 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow<2.3\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KSRCvXy9wr99"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_theme()\n",
        "\n",
        "from sklearn.datasets import make_circles\n",
        "from numpy import where\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.initializers import RandomUniform"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "id": "dmXYPV9fxdJD",
        "outputId": "dee19e6a-4ec0-4de1-9da5-a38f5cadea0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.17.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnZHW-YLwr9-"
      },
      "source": [
        "\n",
        "\n",
        "# Exploding\n",
        "\n",
        "Para essa parte da lista nós preparamos uma rede, note como a *loss* cresce exponencialmente até virar infinita e logo em seguida NaN. Esse é um sintoma classico de exploding gradient. O gradiente está tão elevado que a cada etapa de backpropagation o passo de atualização dos parametros leva a um aumento na *loss* e isso segue crescendo até que exploda.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oXt0hAhUwr9_"
      },
      "outputs": [],
      "source": [
        "def f1(x):\n",
        "    return 5+ 10*x\n",
        "\n",
        "xs = [x for x in range(100)]\n",
        "ys = [f1(x) for x in range(100)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wkm6HLjywr9_",
        "outputId": "40da17d6-8c08-4de2-ad69-b682e5b38ad8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unrecognized data type: x=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] (of type <class 'list'>)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e40f5ade2765>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean_squared_error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/__init__.py\u001b[0m in \u001b[0;36mget_data_adapter\u001b[0;34m(x, y, sample_weight, batch_size, steps_per_epoch, shuffle, class_weight)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unrecognized data type: x={x} (of type {type(x)})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unrecognized data type: x=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] (of type <class 'list'>)"
          ]
        }
      ],
      "source": [
        "opt = keras.optimizers.SGD()\n",
        "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
        "model.compile(optimizer=opt, loss=\"mean_squared_error\")\n",
        "model.fit(xs,ys,epochs=400)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmslLcmcwr-A"
      },
      "source": [
        "## Gradient Cliping\n",
        "## <font color='blue'>Questão 1 </font>\n",
        "\n",
        "\n",
        "Utilizando a mesma arquitetura, realize gradient clipping no otimizador para contornar o problema de exploding gradient, compile e treine seu novo modelo.\n",
        "Você pode usar tanto o parametro `clipvalue` quanto `clipnorm` desde que sua rede consiga minimizar a loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RCs6UXRwr-A"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
        "\n",
        "# Seu código aqui\n",
        "\n",
        "model.fit(xs,ys,epochs=400)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLFY7mXWwr-B"
      },
      "source": [
        "________________________\n",
        "\n",
        "\n",
        "\n",
        "# Vanishing\n",
        "\n",
        "Lidar com Vanishing Gradient é muito mais desafiador do que com exploding gradient. Não é trivial determinar se o baixo desempenho de sua rede é causado por vanishing gradient uma vez que seus sintomas são relativamente genéricos e ele pode ser apenas mais um dos fatores que prejudicam seu desempenho. Além disso não existe uma solução geral e definitiva como o gradient cliping em casos de explosão do gradiente.\n",
        "\n",
        "Preparamos algumas redes para poder explorar um caso mais simples de vanishing gradient e também uma possivel solução. Começamos gerando um dataset simples de classificação e treinamos uma rede rasa que obtem uma boa performance.\n",
        "Ao aprofundarmos essa rede podemos notar que sua performance cai drasticamente se tornando quase tão eficiente quanto jogar uma moeda para chutar a classe do ponto, ela nem mesmo é capaz de \"*overfittar*\" os dados. Utilizaremos o TensorBoard para explorar os gradientes dessa rede mais profunda e tentar analisar essa queda de performance. Em seguida utilizaremos uma nova forma de inicialização dos pesos da rede para tentar recuperar nossa performance e novamente inspecionar os seus gradientes para tentar ganhar algum insight do que está acontecendo dentro dela."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu4V_t_gwr-C"
      },
      "source": [
        "___________\n",
        "\n",
        "## TensorBoard e Callbacks\n",
        "\n",
        "O TensorBoard é uma ferramenta de monitoramento e visualização de redes neurais. Nós o utilizaremos para visualizar os gradientes de nossa rede e poder entender mais afundo o que está causando nosso fraco desempenho. Para mais informações sobre o TensorBoard recomendamos este [guia da documentação oficial](https://www.tensorflow.org/tensorboard/get_started). O TensorBoard pode ser iniciado como parte do notebook usando a \"magica\"  `%load_ext tensorboard` para carrega-lo e `%tensorboard --logdir logs/` para inicia-lo. Outra maneira de utiliza-lo é independente pelo terminal a partir dos logs salvos na pasta `log_dir` com o comando `tensorboard --logdir=\"logs/\"`.\n",
        "\n",
        "Para utilizar o TensorBoard nós utilizamos um callback ao treinar nossa rede.\n",
        "\n",
        "[Callbacks](https://keras.io/api/callbacks/#tensorboard) são objetos do Keras capazes de realizar ações em diferentes etapas do treinamento, como a cada final de epoch ou antes de cada mini-batch. Três exemplos interessates de callbacks são o [EarlyStopping](https://keras.io/api/callbacks/early_stopping/) que te permite encerrar o treinamento de sua rede quando você atinge uma performance minima desejada, o [ModelCheckpoint](https://keras.io/api/callbacks/model_checkpoint/) que lhe permite criar checkpoints do seu modelo para poder reiniciar o treinamento sem perder todo seu progresso em caso de algum problema (como seus gradientes explodindo por exemplo) e o [LearningRateScheduler](https://keras.io/api/callbacks/learning_rate_scheduler/) que lhe permite alterar o learning rate do seu otimizador conforme a epoch de treinamento.\n",
        "\n",
        "O TensorBoard possui um callback que nos permite monitorar o desempenho e parâmetros de nosso modelo ao longo do treinamento, porém para guardar os gradientes da rede necessitamos expandir e adaptar o [callback padrão do TensorBoard](https://keras.io/api/callbacks/tensorboard/).\n",
        "\n",
        "Nós ja implementamos para vocês uma extensão desse callback, para usar este callback basta importa-lo e declara-lo da seguinte maneira:\n",
        "\n",
        "\n",
        "```python\n",
        "log_dir = \"logs/\"\n",
        "tensorboard_callback = ExtendedTensorBoard(x=dados_treino, y=labels_treino,log_dir=log_dir,histogram_freq=1)\n",
        "```\n",
        "________________"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "%load_ext tensorboard\n",
        "\n",
        "class ExtendedTensorBoard(TensorBoard):\n",
        "    \"\"\"\n",
        "    Adaptado de https://github.com/tensorflow/tensorflow/issues/31542\n",
        "    \"\"\"\n",
        "    def __init__(self, x, y,log_dir='logs',\n",
        "                            histogram_freq=0,\n",
        "                            write_graph=True,\n",
        "                            write_images=False,\n",
        "                            update_freq='epoch',\n",
        "                            profile_batch=2,\n",
        "                            embeddings_freq=0,\n",
        "                            embeddings_metadata=None,\n",
        "                            **kwargs,):\n",
        "        self.x=x\n",
        "        self.y=y\n",
        "        super(ExtendedTensorBoard, self).__init__(log_dir,\n",
        "                                                    histogram_freq,\n",
        "                                                    write_graph,\n",
        "                                                    write_images,\n",
        "                                                    update_freq,\n",
        "                                                    profile_batch,\n",
        "                                                    embeddings_freq,\n",
        "                                                    embeddings_metadata,)\n",
        "\n",
        "    def _log_gradients(self, epoch):\n",
        "        writer = self._get_writer(self._train_run_name)\n",
        "        with writer.as_default(), tf.GradientTape() as g:\n",
        "\n",
        "            features=tf.constant(self.x)\n",
        "            y_true=tf.constant(self.y)\n",
        "\n",
        "            y_pred = self.model(features)  # forward-propagation\n",
        "            loss = self.model.compiled_loss(y_true=y_true, y_pred=y_pred)  # calculate loss\n",
        "            gradients = g.gradient(loss, self.model.trainable_weights)  # back-propagation\n",
        "\n",
        "            # In eager mode, grads does not have name, so we get names from model.trainable_weights\n",
        "            for weights, grads in zip(self.model.trainable_weights, gradients):\n",
        "                tf.summary.histogram(\n",
        "                    weights.name.replace(':', '_') + '_grads', data=grads, step=epoch)\n",
        "\n",
        "        writer.flush()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "#         Sobre-escrevemos essa função da super classe pois necessitamos\n",
        "#         adicionar a funcionalidade de gravar os gradientes.\n",
        "#         Como tambem queremos suas funcionalidades originais, tambem invocamos o metodo super\n",
        "        super(ExtendedTensorBoard, self).on_epoch_end(epoch, logs=logs)\n",
        "\n",
        "        if self.histogram_freq and epoch % self.histogram_freq == 0:\n",
        "            self._log_gradients(epoch)\n"
      ],
      "metadata": {
        "id": "T25tiRXE6Z1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc7EGPRFwr-C"
      },
      "source": [
        "## Definindo nossos dados\n",
        "Primeiro definiremos um toy dataset bem simples que utilizaremos para nossos modelos e uma função auxiliar para facilitar a comparação de nossos modelos.\n",
        "\n",
        "\n",
        "Esses dados e redes foram inspirados e adaptados [deste post](https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0uijES3wr-E"
      },
      "outputs": [],
      "source": [
        "# gera dataset de classificação\n",
        "X, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n",
        "\n",
        "# escala input para [-1,1]\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# plota visualização do dataset\n",
        "for i in range(2):\n",
        "    samples_ix = where(y == i)\n",
        "    plt.scatter(X[samples_ix, 0], X[samples_ix, 1], label=str(i))\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# separa em teste e treino\n",
        "n_train = 500\n",
        "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
        "trainy, testy = y[:n_train], y[n_train:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csZjcqGfwr-E"
      },
      "outputs": [],
      "source": [
        "def run_model(model,log_to_tb= False ,trainX=trainX,trainy=trainy,testX=testX,testy=testy):\n",
        "    \"\"\"\n",
        "    Função auxiliar que recebe um modelo e realiza seu treinamento e avaliação no dataset.\n",
        "    \"\"\"\n",
        "    model.summary()\n",
        "\n",
        "    # compila modelo\n",
        "    opt = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "    #Cria log do modelo pra visualizarmos no TensorBoard se a flag estiver ligada\n",
        "    callbacks = None\n",
        "    if log_to_tb==True:\n",
        "        log_dir = \"logs/\" + model.name\n",
        "        callbacks=[ExtendedTensorBoard(x=trainX, y=trainy,log_dir=log_dir,histogram_freq=1)]\n",
        "\n",
        "\n",
        "    # fit modelo\n",
        "    history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=500, verbose=0, callbacks=callbacks)\n",
        "\n",
        "    # avalia modelo\n",
        "    _, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
        "    _, test_acc = model.evaluate(testX, testy, verbose=0)\n",
        "    print(\"\\n\")\n",
        "    print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "\n",
        "    # plota acurácia/training history\n",
        "    plt.ylim(0, 1)\n",
        "    plt.title(\"Acurácia \"+ model.name)\n",
        "    plt.plot(history.history['accuracy'], label='train')\n",
        "    plt.plot(history.history['val_accuracy'], label='test')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XiWOdItwr-F"
      },
      "source": [
        "## Rede rasa\n",
        "Aqui temos uma rede rasa com apenas uma camada oculta e uma de output, note que ela é capaz de atingir uma performance razoavel após 300 epochs.\n",
        "\n",
        "Nos estamos utilizando um inicializador diferente do padrão para os pesos da camada, ele retirar os pesos iniciais a partir de uma distribuição uniforme no intervalo [0,1]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4XQT0w8wr-F"
      },
      "outputs": [],
      "source": [
        "#define modelo raso\n",
        "init = RandomUniform(minval=0, maxval=1)\n",
        "\n",
        "model = keras.Sequential(name=\"modelo_raso\")\n",
        "model.add(keras.layers.Dense(5,\n",
        "                       input_dim=2,\n",
        "                       activation=\"tanh\",\n",
        "                       kernel_initializer=init,\n",
        "                       name=\"raso_1\"))\n",
        "model.add(keras.layers.Dense(1,\n",
        "                       activation='sigmoid',\n",
        "                       kernel_initializer=init,\n",
        "                       name=\"raso_output\"))\n",
        "\n",
        "\n",
        "run_model(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wJ1Y-zFwr-G"
      },
      "source": [
        "## Rede funda\n",
        "\n",
        "Agora tornaremos nossa rede mais funda com 5 camadas ocultas e uma de output, note como a performance cai drasticamente e se torna próxima a um chute aleatório. Embora o modelo seja mais complexo e poderoso nós não conseguimos treina-lo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mz1ncqe6wr-G"
      },
      "outputs": [],
      "source": [
        "# define modelo mais fundo\n",
        "init = RandomUniform(minval=0, maxval=1)\n",
        "\n",
        "model = Sequential(name=\"modelo_fundo\")\n",
        "model.add(Dense(5, input_dim=2, activation='tanh', kernel_initializer=init,name=\"funda_1\"))\n",
        "model.add(Dense(5, activation='tanh', kernel_initializer=init,name=\"funda_2\"))\n",
        "model.add(Dense(5, activation='tanh', kernel_initializer=init,name=\"funda_3\"))\n",
        "model.add(Dense(5, activation='tanh', kernel_initializer=init,name=\"funda_4\"))\n",
        "model.add(Dense(5, activation='tanh', kernel_initializer=init,name=\"funda_5\"))\n",
        "model.add(Dense(1, activation='sigmoid', kernel_initializer=init,name=\"funda_output\"))\n",
        "\n",
        "\n",
        "run_model(model,log_to_tb=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX9jJMCJwr-G"
      },
      "source": [
        "### Utilizando inicialização de Xavier Glorot\n",
        "\n",
        "Agora utilizaremos uma técnica de combate ao vanishing gradient, utilizaremos outro inicializador para os pesos da rede. O [inicializador de Xavier Glorot](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) é o inicializador default de algumas camadas do keras como a camada densa que utilizamos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4FV54uewr-G"
      },
      "outputs": [],
      "source": [
        "# define modelo mais fundo com inicializador de pesos melhor\n",
        "model = Sequential(name=\"modelo_xavier\")\n",
        "model.add(Dense(5, input_dim=2, activation='tanh',kernel_initializer=\"glorot_uniform\", name=\"xavier_1\"))\n",
        "model.add(Dense(5, activation='tanh',kernel_initializer=\"glorot_uniform\", name=\"xavier_2\"))\n",
        "model.add(Dense(5, activation='tanh',kernel_initializer=\"glorot_uniform\", name=\"xavier_3\"))\n",
        "model.add(Dense(5, activation='tanh', kernel_initializer=\"glorot_uniform\",name=\"xavier_4\"))\n",
        "model.add(Dense(5, activation='tanh',kernel_initializer=\"glorot_uniform\", name=\"xavier_5\"))\n",
        "model.add(Dense(1, activation='sigmoid', kernel_initializer=\"glorot_uniform\", name=\"xavier_output\"))\n",
        "\n",
        "run_model(model,log_to_tb=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jcpvxhw-wr-G"
      },
      "source": [
        "## <font color='blue'>Questão 2 </font>\n",
        "Inicialize o TensorBoard e cheque os histogramas e as distribuições dos valores dos gradientes das 2 redes, como eles se diferem de uma rede para outra? Preste atenção em particular para a diferença de amplitude dos seus valores. Que conclusão podemos tirar? Insira/cole imagens do TensorBoard para basear seus argumentos, pode ser uma simples captura de tela.\n",
        "\n",
        "**Obs:** Você pode filtrar elementos ao escrever a tag \"kernel_0_grads\" como filtro para facilitar sua exploração dos gradientes dos pesos excluindo os termos de bias/viés.\n",
        "\n",
        "**Obs2:** Lembre-se de apagar a pasta `logs` se você for retreinar alguma das redes, caso contrário ele irá dar um append nos seus 2 treinamentos e seus graficos de loss e acurácia ficarão errados com loops ligando o começo de um treinamento ao final do outro.\n",
        "\n",
        "**<font color='red'> Sua resposta aqui </font>**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "cljhcqC7y7AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-8pvvM5C18Sd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv_tutorial",
      "language": "python",
      "name": ".venv_tutorial"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Copy of Lista 05 - Vanishing&Exploding Gradient.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}